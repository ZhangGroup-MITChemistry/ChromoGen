{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2803a-d0f5-4939-8f11-a6d149c76312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/state/partition1/llgrid/pkg/anaconda/python-LLM-2023b/bin/python\n",
    "\n",
    "#SBATCH --job-name=generate_embeddings\n",
    "##SBATCH --partition=debug-gpu\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH -c 40\n",
    "##SBATCH -t 0-0:05\n",
    "##SBATCH -t 0-36:00:00\n",
    "#SBATCH --output=./log_files/generate_embeddings.log\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.insert(1,'./')\n",
    "from SequencesDataset import SequencesDataset\n",
    "from ConfigDataset import ConfigDataset\n",
    "sys.path.insert(2,'../frontend/')\n",
    "from Tranmodel import Tranmodel \n",
    "\n",
    "####################################################################\n",
    "# Select the regions to be processed \n",
    "####################################################################\n",
    "\n",
    "# We want to process all chromosomes. \n",
    "chroms = [*range(1,23),'X']\n",
    "\n",
    "# This is where the embeddings will be saved \n",
    "save_folder = '../../data/embeddings/'\n",
    "dest_fp = lambda chrom: save_folder+f'chrom_{chrom}.tar.gz'#'../../data/processed_data.hdf5'\n",
    "if not os.path.exists(save_folder): \n",
    "    os.mkdir(save_folder)\n",
    "\n",
    "##################################\n",
    "# Use the ConfigDataset class to identify\n",
    "# the genomic regions whose embeddings are of interest to us. \n",
    "print('Determining which genomic regions must be embedded.\\n',flush=True)\n",
    "def get_genomic_regions(\n",
    "    nbeads = 65,\n",
    "    dataset_filepath = '../../data/processed_data.hdf5',\n",
    "    batch_size = 64,\n",
    "    two_channels = False,\n",
    "    allow_overlap = True, \n",
    "    chroms=None\n",
    "):\n",
    "    \n",
    "    cds = ConfigDataset(\n",
    "        dataset_filepath,\n",
    "        segment_length=nbeads,\n",
    "        batch_size=batch_size,\n",
    "        normalize_distances=True,\n",
    "        geos=None,\n",
    "        organisms=None,\n",
    "        cell_types=None,\n",
    "        cell_numbers=None,\n",
    "        chroms=chroms,\n",
    "        replicates=None,\n",
    "        shuffle=True,\n",
    "        allow_overlap=allow_overlap, \n",
    "        two_channels=two_channels,\n",
    "        try_GPU=True,\n",
    "        mean_dist_fp='../../data/mean_dists.pt',\n",
    "        mean_sq_dist_fp='../../data/squares.pt'\n",
    "    )\n",
    "    \n",
    "    return cds.get_genomic_regions()\n",
    "    \n",
    "regions = get_genomic_regions()\n",
    "\n",
    "# Rename some columns to match what had previously been used downstream\n",
    "# to avoid having to rewrite code. \n",
    "regions = regions.rename(\n",
    "    columns={\n",
    "        'Chromosome':'Chromosome',\n",
    "        'Start':'Genomic_Index',\n",
    "        'Stop':'Region_Length'\n",
    "    }\n",
    ")\n",
    "regions['Region_Length']-= regions['Genomic_Index']\n",
    "idx_cols = ['Region_Length','Chromosome','Genomic_Index'] # for later\n",
    "\n",
    "##################################\n",
    "# Load the pretrained frontend, which generates the embeddings \n",
    "frontend = Tranmodel.get_pretrained_model()\n",
    "try: \n",
    "    frontend.cuda()\n",
    "except: \n",
    "    pass \n",
    "\n",
    "for chrom in chroms:\n",
    "    break\n",
    "    print('/////////////////////////////////////////////////////////////////')\n",
    "    if os.path.exists(dest_fp(chrom)):\n",
    "        print(f'Chromosome {chrom} already processed!\\n\\n',flush=True)\n",
    "        continue\n",
    "    print(f'Chromosome {chrom}:\\n',flush=True)\n",
    "    # Load the raw sequencing data for this chromosome\n",
    "    chr = f'chr{chrom}'\n",
    "    ds = SequencesDataset(chroms=chr)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Get the genomic regions relevant to this chromosome\n",
    "    embed_df = regions[regions['Chromosome'] == str(chrom)].reset_index(drop=True)\n",
    "    \n",
    "    # Form the embeddings\n",
    "    data = []\n",
    "    idx_to_keep = np.ones(len(embed_df),dtype=bool)\n",
    "    for i in tqdm(range(len(embed_df)), desc = f'Embedding Generation Progress (Chromosome {chrom})', total = len(embed_df)): # Show progress\n",
    "        try: \n",
    "            data.append( frontend( # Generate embeddings of sequencing data\n",
    "                ds.fetch([ (chr,embed_df.loc[i,'Genomic_Index'],embed_df.loc[i,'Region_Length']) ]).to(frontend.device) # Prepare sequencing data\n",
    "            ).to('cpu') )\n",
    "        except: \n",
    "            idx_to_keep[i] = False\n",
    "    embed_df = embed_df[idx_to_keep]\n",
    "    embed_df['Data'] = data\n",
    "    del data \n",
    "    '''\n",
    "    embed_df['Data'] = [\n",
    "        frontend( # Generate embeddings of sequencing data\n",
    "            ds.fetch([ (chr,embed_df.loc[i,'Genomic_Index'],embed_df.loc[i,'Region_Length']) ]).to(frontend.device) # Prepare sequencing data\n",
    "        ).to('cpu') for i in tqdm(range(len(embed_df)), desc = f'Embedding Generation Progress (Chromosome {chrom})', total = len(embed_df)) # Show progress\n",
    "    ]\n",
    "    '''\n",
    "    '''\n",
    "    print(f'Generating Embeddings for Chromosome {chrom}',flush=True)\n",
    "    embed_df['Data'] = [\n",
    "        frontend( # Generate embeddings of sequencing data\n",
    "            ds.fetch([ (chr,embed_df.loc[i,'Genomic_Index'],embed_df.loc[i,'Region_Length']) ]).to(frontend.device) # Prepare sequencing data\n",
    "        ).to('cpu') for i in range(len(embed_df))\n",
    "    ]\n",
    "    '''\n",
    "    '''\n",
    "    for i in range(len(embed_df)): \n",
    "        raw = ds.fetch([ (chr,embed_df.loc[i,'Genomic_Index'],embed_df.loc[i,'Region_Length']) ]).to(frontend.device)\n",
    "        embeddings.append( frontend(raw).to('cpu') )\n",
    "    '''\n",
    "    # Clear RAM \n",
    "    del ds, chr\n",
    "\n",
    "    # Create a MultiIndex for easy fetching of specific embeddings in the DataLoader\n",
    "    embed_df.index = pd.MultiIndex.from_tuples(\n",
    "        list(map(tuple,embed_df[idx_cols].values)),\n",
    "        names=idx_cols\n",
    "    )\n",
    "\n",
    "    # Drop the now-redundant columns\n",
    "    embed_df = embed_df.drop(columns=idx_cols)\n",
    "\n",
    "    # Save this to streamlined tar.gz file. \n",
    "    print(f'Saving Embeddings for Chromosome {chrom}',flush=True)\n",
    "    embed_df.to_pickle(dest_fp(chrom))\n",
    "\n",
    "    # Free RAM \n",
    "    del embed_df\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54976df-63a1-4c0f-a64d-db86694f57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.insert(1,'./')\n",
    "from SequencesDataset import SequencesDataset\n",
    "from ConfigDataset import ConfigDataset\n",
    "sys.path.insert(2,'../frontend/')\n",
    "from Tranmodel import Tranmodel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73183be7-ff64-4a26-876a-5b933dda048f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "../../data/outside/GM12878_hg38.pkl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mchr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mSequencesDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchroms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43malignment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhg38\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/refining_scHiC/revamp_with_zhuohan/code/data_utils/SequencesDataset.py:84\u001b[0m, in \u001b[0;36mSequencesDataset.__init__\u001b[0;34m(self, cell_type, alignment, data_dir, resolution, region_length, batch_size, chroms)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenome_fps[chrom] \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutside/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malignment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchrom\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenome_fps[chrom]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenome_fps[chrom]\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnase_fp), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnase_fp\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading sequencing data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: ../../data/outside/GM12878_hg38.pkl"
     ]
    }
   ],
   "source": [
    "chr = 1 \n",
    "\n",
    "ds = SequencesDataset(chroms=chr,alignment='hg38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabeba90-f94b-4673-82e4-e0f2bdc53ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GM12878_hg19.mcool      GM12878_hg19_1000_backup.cool  GM12878_hg38.mcool\n",
      "GM12878_hg19.pkl        GM12878_hg19_backup.mcool      \u001b[0m\u001b[01;34mhg19\u001b[0m/\n",
      "GM12878_hg19_1000.cool  GM12878_hg19_full.mcool        \u001b[01;34mhg38\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../../data/outside/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb9c317-35c1-48aa-a073-41bc0323966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =pd.read_pickle('../../data/outside/GM12878_hg19.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0d79c8-f44d-4f5b-a6c6-50bde5d56d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: <1x249250000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 151815820 stored elements in Compressed Sparse Row format>,\n",
       " 10: <1x135534000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 88346740 stored elements in Compressed Sparse Row format>,\n",
       " 11: <1x135006000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 90595820 stored elements in Compressed Sparse Row format>,\n",
       " 12: <1x133851000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 89551740 stored elements in Compressed Sparse Row format>,\n",
       " 13: <1x115169000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 63193920 stored elements in Compressed Sparse Row format>,\n",
       " 14: <1x107349000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 59223540 stored elements in Compressed Sparse Row format>,\n",
       " 15: <1x102531000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 53820000 stored elements in Compressed Sparse Row format>,\n",
       " 16: <1x90354000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 51812300 stored elements in Compressed Sparse Row format>,\n",
       " 17: <1x81195000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 52171015 stored elements in Compressed Sparse Row format>,\n",
       " 18: <1x78077000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 50762720 stored elements in Compressed Sparse Row format>,\n",
       " 19: <1x59128000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 36300400 stored elements in Compressed Sparse Row format>,\n",
       " 2: <1x243199000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 161217260 stored elements in Compressed Sparse Row format>,\n",
       " 20: <1x63025000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 42057180 stored elements in Compressed Sparse Row format>,\n",
       " 21: <1x48129000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 22723040 stored elements in Compressed Sparse Row format>,\n",
       " 22: <1x51304000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 22962960 stored elements in Compressed Sparse Row format>,\n",
       " 3: <1x198022000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 133783540 stored elements in Compressed Sparse Row format>,\n",
       " 4: <1x191154000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 123265960 stored elements in Compressed Sparse Row format>,\n",
       " 5: <1x180915000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 119091740 stored elements in Compressed Sparse Row format>,\n",
       " 6: <1x171115000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 114771160 stored elements in Compressed Sparse Row format>,\n",
       " 7: <1x159138000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 101772580 stored elements in Compressed Sparse Row format>,\n",
       " 8: <1x146364000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 96505160 stored elements in Compressed Sparse Row format>,\n",
       " 9: <1x141213000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 74964580 stored elements in Compressed Sparse Row format>,\n",
       " 'X': <1x155270000 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 91341620 stored elements in Compressed Sparse Row format>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5578175-314e-4cc5-959e-03eda85bf90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
