{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d0e4c8-4e94-488c-845a-e5447441a55f",
   "metadata": {},
   "source": [
    "#### Figs 2a-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e48f9d-86f5-41d4-881d-8fccbb02d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 10:28:01.624204: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import ChromoGen\n",
    "from ChromoGen.model.Diffuser.training_utils import ConfigDataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../support/plot_style.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e6e31-eed5-41c6-adc0-5d990550ba50",
   "metadata": {},
   "source": [
    "Calculation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b309ec-11cb-48a1-990a-ff31aa607829",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_separation = 1    # Min bond separation to analyze\n",
    "maximum_separation = 63   # Max bond separation to analyze\n",
    "resolution = 20_000       # in bp \n",
    "batch_size = 2_000_000    # Largest number of conformations to analyze simultaneously. \n",
    "gpu_calculations_ok = True \n",
    "max_workers = None        # executor-associated threads. Torch will multithread in its default manner regardless. \n",
    "calc_dtype = torch.double"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95db80-9226-48f7-82f8-9eaeba6f178f",
   "metadata": {},
   "source": [
    "Verify GPU is available if requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa80b82f-773d-4cb3-9e35-0e8b25acd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_calculations_ok = gpu_calculations_ok and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ce5c8-6deb-4eda-8786-164961d79e1e",
   "metadata": {},
   "source": [
    "Data locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba591d1-e661-4fc8-8919-97e72fcb8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Conformations \n",
    "## Directory containing the GEO-acquired Dip-C 3DG files\n",
    "formatted_dipc_data = '../../downloaded_data/conformations/DipC/processed_data.h5'\n",
    "#formatted_dipc_data = '../../outside_data/dipc/processed_data.h5' # If you reproduced the data\n",
    "\n",
    "# Path to the DCD file produced by the LAMMPS homopolymer simulation\n",
    "dcd_fp = '../../downloaded_data/conformations/MDHomopolymer/DUMP_FILE.dcd'\n",
    "#dcd_fp = '../../generate_data/conformations/homopolymer/DUMP_FILE.dcd' # If you reproduced the data\n",
    "\n",
    "# Directory containing GM12878 genome-wide conformations from ChromoGen\n",
    "genome_wide_data_dir = '../../downloaded_data/conformations/ChromoGen/genome_wide/GM12878/'\n",
    "#genome_wide_data_dir = '../../generate_data/conformations/genome_wide/GM12878/' # If you reproduced the data\n",
    "\n",
    "# Save directories\n",
    "data_dir = './temp_data/' # Directory in which to save the processed data\n",
    "pdf_dir = './pdfs/'       # Directory in which to save the figure PDFs\n",
    "\n",
    "# File with indices to use. It's ok if it doesn't exist; it'll be generated below if need be. \n",
    "start_indices_fp = '../../generate_data/conformations/genome_wide_indices.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf67535-78f9-40f6-9d16-6e38f07c0d41",
   "metadata": {},
   "source": [
    "Just gonna... check the paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2323964-99f9-4c4f-ac10-5307bc35bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = Path(pdf_dir)\n",
    "pdf_dir.mkdir(exist_ok=True, parents=True)\n",
    "data_dir = Path(data_dir)\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "start_indices_fp = Path(start_indices_fp)\n",
    "\n",
    "formatted_dipc_data = Path(formatted_dipc_data)\n",
    "dipc_data_fp = data_dir / 'dipc_dists.pkl'\n",
    "dipc_rg_fp = data_dir / 'dipc_R_g.pt'\n",
    "if not dipc_data_fp.exists():\n",
    "    assert formatted_dipc_data.exists(), (\n",
    "        'The formatted dipc data seems not to have been found. '\n",
    "        'You can generate it by running the script `ChromoGen/recreate_results/outside_data/dipc/download.py` '\n",
    "        'followed by the script `ChromoGen/recreate_results/outside_data/dipc/process_raw_data.py`. Or download '\n",
    "        'our data from the relevant URL.'\n",
    "    )\n",
    "\n",
    "dcd_fp = Path(dcd_fp)\n",
    "homopolymer_data_fp = data_dir / 'homopolymer_dists.pkl'\n",
    "homo_rg_fp = data_dir / 'homopolyer_R_g.pt'\n",
    "if not homopolymer_data_fp.exists():\n",
    "    assert dcd_fp.exists(), (\n",
    "        'The LAMMPS-generated .dcd file was not found. '\n",
    "        'You can generate it by navigating to `../../generate_data/conformations/MDHomopolymer/` '\n",
    "        'and running `job.pbs` (you\\'ll have to change some filepaths) then `find_fixed_T_traj.py`. '\n",
    "        'Or download our data from the relevant URL.'\n",
    "    )\n",
    "\n",
    "genome_wide_data_dir = Path(genome_wide_data_dir)\n",
    "genome_wide_data_fp = (data_dir / 'chromogen_dists.pkl')\n",
    "cgen_rg_fp = data_dir / 'chromogen_R_g.pt'\n",
    "if not genome_wide_data_fp.exists():\n",
    "    assert (\n",
    "        genome_wide_data_dir.exists() and\n",
    "        len(list(genome_wide_data_dir.rglob('*.pt'))) == 2658 * 2 # 2658 regions, two guidance strengths\n",
    "    ), (\n",
    "        'The ChromoGen-generated conformations were not found. You can generate them by '\n",
    "        'running the script `../../generate_data/conformations/genome_wide.py`. You\\'ll '\n",
    "        'have to set your own SLURM settings. Only GM12878 structures are required here, so '\n",
    "        'you can also change `#SBATCH --array=0-5` to `#SBATCH --array=0-2`. '\n",
    "        'Alternatively, you can download our data at the relevant URL.'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db6ddb-ef9a-4ec4-9c02-4ceeee0cd692",
   "metadata": {},
   "source": [
    "## Compute the distances in each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236562a8-2c71-4e8b-a330-228cce812130",
   "metadata": {},
   "source": [
    "### Compute pairwise distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cc443-2c41-49cb-85a5-fb65471dd616",
   "metadata": {},
   "source": [
    "#### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e668ecb-eea4-4b7c-a922-3b0674d722d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_by_separation(\n",
    "    coords,sep_dists=None,\n",
    "    min_sep=minimum_separation,\n",
    "    max_sep=maximum_separation,\n",
    "    dtype = calc_dtype\n",
    "):\n",
    "    assert min_sep > 0, 'min_sep should be positive-valued'\n",
    "    if max_sep is None:\n",
    "        max_sep = coords.num_beads - 1\n",
    "    else:\n",
    "        assert max_sep < coords.num_beads, (\n",
    "            f'Max separation, max_sep={max_sep}, is out of bounds for polymer with '\n",
    "            f'{coords.num_beads} beads/monomers.'\n",
    "        )\n",
    "    assert min_sep <= max_sep, \\\n",
    "    'Minimum separation must be less than or equal to maximum separation, but min_sep > max_sep.'\n",
    "    if sep_dists is None:\n",
    "        sep_dists = {}\n",
    "\n",
    "    # Use the specified dtype during computation and move to GPU if desired. \n",
    "    coords = coords.to(dtype)\n",
    "    \n",
    "    for sep in range(min_sep,max_sep+1):\n",
    "        if sep == 0:\n",
    "            a = b = coords\n",
    "        else:\n",
    "            a = coords.values[...,:-sep,:]\n",
    "            b = coords.values[...,sep:,:]\n",
    "\n",
    "        pairwise_dists = (a-b).square_().sum(-1).sqrt_()\n",
    "\n",
    "        # Remove NaN values & move to the CPU memory. \n",
    "        # This also flattens the object. \n",
    "        pairwise_dists = pairwise_dists[pairwise_dists.isfinite()].cpu()\n",
    "\n",
    "        # Append to the dataset if necessary. Otherwise, create a new entry\n",
    "        if (pd1:= sep_dists.get(sep)) is not None:\n",
    "            sep_dists[sep] = torch.cat([pd1, pairwise_dists])\n",
    "        else:\n",
    "            sep_dists[sep] = pairwise_dists\n",
    "            \n",
    "    return sep_dists\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d1b25-7166-436e-ac06-ac1387284d28",
   "metadata": {},
   "source": [
    "#### Dip-C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc43cda-6ef7-4c17-8ffb-093d2649fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dipc_conformation(f,resolution=20_000):\n",
    "\n",
    "    ####\n",
    "    # Initial prep \n",
    "    # Load the data\n",
    "    df = pd.read_csv(\n",
    "        f,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=['Chromosome','Genomic_Index','x','y','z']\n",
    "    )\n",
    "\n",
    "    # A number of these files duplicate rows for some reason... so drop\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Not really an issue AFAIK, but JUST IN CASE, let's make sure everything \n",
    "    # is properly sorted\n",
    "    df = df.sort_values(['Chromosome','Genomic_Index'],axis='index',ignore_index=True)\n",
    "\n",
    "    ####\n",
    "    # Make a the Conformations object from the data\n",
    "\n",
    "    # Get dimensionality (Note that chromosome labels \n",
    "    # distinguish between each copy of each chromosome, so\n",
    "    # this does, indeed, separate polymers.) \n",
    "    chroms = df['Chromosome'].unique()\n",
    "    n_chroms = len(chroms)\n",
    "    max_len = df['Genomic_Index'].max() // resolution + 1\n",
    "\n",
    "    # Initialize a torch.Tensor. \n",
    "    # Note that values removed in the cleaning process are simply omitted\n",
    "    # in the 3DG file. We'll insert NaN values to simplify downstream analysis.\n",
    "    coords = torch.empty(n_chroms, max_len, 3, dtype=torch.double)\n",
    "\n",
    "    for k,chrom in enumerate(chroms):\n",
    "        df1 = df[df['Chromosome'] == chrom]\n",
    "        coords[k,df1['Genomic_Index'].values//resolution,:] = torch.from_numpy(\n",
    "            df1[['x','y','z']].values\n",
    "        )\n",
    "    return ChromoGen.Conformations(coords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d57f5e-2273-4587-909d-511915d4d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dipc_data_fp.exists() or not dipc_rg_fp.exists():\n",
    "\n",
    "    # Note that, while we use partially overlapping regions in the generated dataset, \n",
    "    # we use nonoverlapping regions here. This offsets the regions a bit to make \n",
    "    # so that the regions aren't identical, which itself could bias the results in \n",
    "    # ChromoGen's favor since the homopolymer simulation didn't target any specific\n",
    "    # set of regions. \n",
    "    config_ds = ConfigDataset(\n",
    "        str(formatted_dipc_data),\n",
    "        segment_length=maximum_separation+1,\n",
    "        remove_diagonal=False,\n",
    "        batch_size=0,\n",
    "        normalize_distances=False, \n",
    "        geos=None,\n",
    "        organisms=None,\n",
    "        cell_types=None,\n",
    "        cell_numbers=None,\n",
    "        chroms=None, \n",
    "        replicates=None,\n",
    "        shuffle=False,\n",
    "        allow_overlap=False, # the class ensures no overlap, so we can just use its start indices\n",
    "        two_channels=False,\n",
    "        try_GPU=False,\n",
    "        mean_dist_fp=None,\n",
    "        mean_sq_dist_fp=None\n",
    "    )\n",
    "\n",
    "    # Get all the non-overlapping conformations\n",
    "    si = config_ds.start_indices\n",
    "    c = config_ds.coords\n",
    "    dipc_coords = torch.stack(\n",
    "        [\n",
    "            c[si+k,:] for k in range(maximum_separation+1) \n",
    "        ],\n",
    "        dim=-2\n",
    "    )\n",
    "\n",
    "    # Separate maternal/paternal data\n",
    "    dipc_coords = ChromoGen.Conformations(\n",
    "        torch.cat(\n",
    "            [\n",
    "                dipc_coords[...,:3], # Maternal\n",
    "                dipc_coords[...,3:], # Paternal\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "    ).flatten()\n",
    "\n",
    "    # Compute the distance data\n",
    "    radii_of_gyration = []\n",
    "    dipc_dists = {}\n",
    "    i = 0\n",
    "    while i < len(dipc_coords):\n",
    "        c = dipc_coords[i:i+batch_size]\n",
    "        if gpu_calculations_ok:\n",
    "            c = c.cuda()\n",
    "        dipc_dists = distances_by_separation(c,dipc_dists)\n",
    "        #radii_of_gyration.append(c.trajectory.compute_rg()) # The Trajectory class uses MDTraj's R_g method here\n",
    "        radii_of_gyration.append(c.compute_rg().cpu()) \n",
    "        i+= batch_size\n",
    "    radii_of_gyration = torch.cat(radii_of_gyration)\n",
    "        \n",
    "    # Save the results & free up some RAM\n",
    "    pickle.dump(dipc_dists,dipc_data_fp.open('wb'))\n",
    "    torch.save(radii_of_gyration,dipc_rg_fp)\n",
    "    dipc_dists.clear()\n",
    "    del dipc_dists, config_ds, dipc_coords, si, c, radii_of_gyration\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6fdc7-144c-4a54-83ca-a19fa890e77f",
   "metadata": {},
   "source": [
    "#### MD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b8203-73e2-406a-88e6-659691aed6e5",
   "metadata": {},
   "source": [
    "Load the dcd file produced by LAMMPS & process the data. The simulation had 500 monomers, but we only analyze the central $\\texttt{maximum\\_separation}+1$ beads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d14397-24da-4da5-9868-b485c5b49cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not homopolymer_data_fp.exists() or not homo_rg_fp.exists():\n",
    "    \n",
    "    i = (500 - maximum_separation - 1) // 2\n",
    "    j = i + maximum_separation + 1\n",
    "\n",
    "    coords = ChromoGen.Conformations(dcd_fp,representation='coordinates',num_beads=500)[...,i:j,:]\n",
    "\n",
    "    homo_distance_stats = {}\n",
    "    radii_of_gyration = []\n",
    "    i = 0\n",
    "    while i < len(coords):\n",
    "        c = coords[i:i+batch_size,...]\n",
    "        if gpu_calculations_ok:\n",
    "            c = c.cuda()\n",
    "        homo_distance_stats = distances_by_separation(c,homo_distance_stats)\n",
    "        #radii_of_gyration.append(c.trajectory.compute_rg()) # The Trajectory class uses MDTraj's R_g method here. The paper used this\n",
    "        radii_of_gyration.append(c.compute_rg().cpu())  # Using the custom calc here though because it's way faster. You can compare to the paper's results\n",
    "        del c\n",
    "        i+= batch_size\n",
    "    radii_of_gyration = torch.cat(radii_of_gyration)\n",
    "    \n",
    "    ####\n",
    "    # Save the statistics\n",
    "    pickle.dump(homo_distance_stats, homopolymer_data_fp.open('wb'))\n",
    "    torch.save(radii_of_gyration,homo_rg_fp)\n",
    "    homo_distance_stats.clear()\n",
    "    del homo_distance_stats, radii_of_gyration, coords\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458167cc-57f5-452b-9856-bc23726111be",
   "metadata": {},
   "source": [
    "#### ChromoGen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7c152-0d65-487f-895d-fa94a158fa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35060a63eb2644aa9ae4997304f1bb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submit load jobs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e895b6cfc254123a5350bee17ca8da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetch/process data:   0%|          | 0/5316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if True:#not genome_wide_data_fp.exists():\n",
    "    # Load all the data. We only used 200 conformations from each region for this part. \n",
    "    # Because we have two data files per region -- one for each guidance weight --\n",
    "    # we'll only take 100 conformations from each file. Also, some files contain\n",
    "    # NaN values, so we'll drop those. \n",
    "\n",
    "    ###################################################################################################\n",
    "    # Identify the partially overlapping regions for which we have Dip-C data available.\n",
    "    ''' # Actually, idk why I did this. The files define this. Leaving for reference though\n",
    "    if start_indices_fp.exists():\n",
    "        start_indices = pd.read_pickle(start_indices_fp)\n",
    "    else:\n",
    "        # Load all of the pre-processed conformations\n",
    "        # Need to analyze the Tan data again\n",
    "        config_ds = ConfigDataset(\n",
    "            str(formatted_dipc_data),\n",
    "            segment_length=65, # Have to hard code this since it affects the selected start indices, which we may compute below\n",
    "            remove_diagonal=False,\n",
    "            batch_size=0,\n",
    "            normalize_distances=False, \n",
    "            geos=None,\n",
    "            organisms=None,\n",
    "            cell_types=None,\n",
    "            cell_numbers=None,\n",
    "            chroms=None, \n",
    "            replicates=None,\n",
    "            shuffle=False,\n",
    "            allow_overlap=True, \n",
    "            two_channels=False,\n",
    "            try_GPU=False,\n",
    "            mean_dist_fp=None,\n",
    "            mean_sq_dist_fp=None\n",
    "        )\n",
    "        \n",
    "        # Construct the 'rosetta' stone that is sometimes referred to in other code\n",
    "        rosetta = {}\n",
    "        si = torch.from_numpy(config_ds.start_indices)\n",
    "        gi = torch.from_numpy(config_ds.genomic_index)\n",
    "        \n",
    "        for _, row in config_ds.coord_info.iterrows():\n",
    "            chrom = row.iloc[-3]\n",
    "            idx_min = row.iloc[-2]\n",
    "            idx_max = row.iloc[-1]\n",
    "            starts = gi[si[ (idx_min <= si) & (si <= idx_max) ]]\n",
    "            if (d:= rosetta.get(chrom)) is not None:\n",
    "                starts = torch.cat([d, starts])\n",
    "            rosetta[chrom] = starts\n",
    "        \n",
    "        for chrom, starts in rosetta.items():\n",
    "            rosetta[chrom] = starts.unique().sort().values\n",
    "            if chrom == '7':\n",
    "                # For some reason, Zhuohan couldn't create the embedding \n",
    "                # for the last index here. \n",
    "                rosetta[chrom] = rosetta[chrom][:-1] \n",
    "\n",
    "        # Choose the partially overlapping start indices\n",
    "        start_indices = {}\n",
    "        \n",
    "        for chrom in rosetta:\n",
    "            indices = []\n",
    "            next_permitted_start = -1\n",
    "            for i,genome_idx in enumerate(rosetta[chrom]):\n",
    "                if genome_idx >= next_permitted_start:\n",
    "                    indices.append((i,genome_idx))\n",
    "                    next_permitted_start = genome_idx + 1_030_000\n",
    "        \n",
    "            start_indices[chrom] = indices\n",
    "\n",
    "        start_indices_fp.parent.mkdir(exist_ok=True,parents=True)\n",
    "        pickle.dump(start_indices,start_indices_fp.open('wb'))\n",
    "    '''\n",
    "    ###################################################################################################\n",
    "    # Load the conformations/perform the computation\n",
    "\n",
    "    def load_coords(f):\n",
    "        # 100 because 200 in each region, 2 files per region from the two \n",
    "        # guidance strengths\n",
    "\n",
    "        # Generation is stochastic, so the first 100 conformations are as independent\n",
    "        # as any other subset\n",
    "        return ChromoGen.Conformations(f,drop_invalid_conformations=True)[:100]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "\n",
    "        # Load the conformations. There are like 5000 files, so \n",
    "        # I'm loading in parallel to help with the IO. \n",
    "        # You may want to set max_workers to 1 if you aren't on a \n",
    "        # distributed system though. \n",
    "        coord_futures = []\n",
    "        for fp in tqdm(genome_wide_data_dir.rglob('*.pt'),desc='Submit load jobs'):\n",
    "            coord_futures.append(executor.submit(load_coords,fp))\n",
    "\n",
    "        pbar = tqdm(initial=0, total=len(coord_futures), desc = 'Fetch/process data')\n",
    "        radii_of_gyration = []\n",
    "        chromogen_dists = {}\n",
    "        while coord_futures:\n",
    "            i=0\n",
    "            coords = []\n",
    "            while coord_futures and i < batch_size:\n",
    "                coords.append(coord_futures.pop(0).result())\n",
    "                i+=100 # 100 conformations taken from each file\n",
    "                pbar.update(1)\n",
    "            coords = coords[0].extend(coords[1:])\n",
    "            if gpu_calculations_ok:\n",
    "                coords = coords.cuda()\n",
    "            chromogen_dists = distances_by_separation(coords,chromogen_dists)\n",
    "            #radii_of_gyration.append(coords.trajectory.compute_rg())\n",
    "            radii_of_gyration.append(coords.compute_rg().cpu()) \n",
    "            del coords\n",
    "        coord_futures.clear()\n",
    "        del coord_futures\n",
    "    radii_of_gyration = torch.cat(radii_of_gyration)\n",
    "\n",
    "    # Save the result\n",
    "    pickle.dump(chromogen_dists,genome_wide_data_fp.open('wb'))\n",
    "    torch.save(radii_of_gyration,cgen_rg_fp)\n",
    "    chromogen_dists.clear()\n",
    "    del chromogen_dists, radii_of_gyration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cb662-408c-4456-8f10-0a9f073225b8",
   "metadata": {},
   "source": [
    "## Compare and plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db2eff-4dd6-425c-94e4-5fae061a5289",
   "metadata": {},
   "source": [
    "#### Load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21885cbe-365f-401d-85fb-18115f53b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_dists = pickle.load(homopolymer_data_fp.open('rb'))\n",
    "cgen_dists = pickle.load(genome_wide_data_fp.open('rb'))\n",
    "dipc_dists = pickle.load(dipc_data_fp.open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e05e65-ec6b-4465-8874-c82d1633ca55",
   "metadata": {},
   "source": [
    "#### Compute the KL divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b0b1c-3dc5-4bb8-9afe-2cb6b24c349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KL_div(exp_dists,ref_dists,num_bins=None):#100):\n",
    "\n",
    "\n",
    "    # Determine number of bins to use\n",
    "    if num_bins is None:\n",
    "        n = min( exp_dists.numel(), ref_dists.numel() )\n",
    "        num_bins = int( 2 * n**(1/3) )\n",
    "        # from rice, per https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width\n",
    "    \n",
    "    # Choose the bins\n",
    "    maxx = ref_dists.max()\n",
    "    delta = maxx/num_bins\n",
    "    bins = torch.arange(0,maxx+delta/2,delta,dtype=exp_dists.dtype)\n",
    "\n",
    "    # Bin the data\n",
    "    exp_probs = torch.histogram(exp_dists,bins).hist\n",
    "    ref_probs = torch.histogram(ref_dists,bins).hist\n",
    "\n",
    "    # Normalize \n",
    "    exp_probs/= exp_probs.sum()\n",
    "    ref_probs/= ref_probs.sum()\n",
    "    \n",
    "    # Avoid NaN values\n",
    "    idx = torch.where( (exp_probs!=0) & (ref_probs!=0) )[0]\n",
    "    exp_probs = exp_probs[idx]\n",
    "    ref_probs = ref_probs[idx]\n",
    "\n",
    "    # Compute the KL-divergence with expectation probabilities from tan\n",
    "    # using ln here reports units of nats\n",
    "    return (exp_probs * (exp_probs.log() - ref_probs.log())).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab92815-8123-4fbc-899a-ba77352caf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgen_KL = torch.tensor([get_KL_div(cgen_dists[separation],dipc_dists[separation]) for separation in cgen_dists])\n",
    "homo_KL = torch.tensor([get_KL_div(homo_dists[separation],dipc_dists[separation]) for separation in homo_dists])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5622b-97b6-46db-adba-0be1b3f78af8",
   "metadata": {},
   "source": [
    "#### Actually plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae9de0-86f3-4a92-bd8e-cfec04a0cddd",
   "metadata": {},
   "source": [
    "#### Figs 2a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf3a9b-8f45-4548-8068-6bca2f464688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dists(\n",
    "    sep,\n",
    "    cgen_dists=cgen_dists,\n",
    "    dipc_dists=dipc_dists,\n",
    "    homo_dists=homo_dists,\n",
    "    fig=None,\n",
    "    ax=None,\n",
    "    nbins=100,\n",
    "    **kwargs\n",
    "):\n",
    "\n",
    "    assert sep > 0\n",
    "    \n",
    "    # Select the relevant data. Multiple by 100 to convert to nm\n",
    "    c = cgen_dists[sep].flatten() * 100 \n",
    "    t = dipc_dists[sep].flatten() * 100\n",
    "    h = homo_dists[sep].flatten() * 100\n",
    "\n",
    "    # Prep the figure if necessary\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    # Choose bins\n",
    "    if 'bins' in kwargs:\n",
    "        bins = kwargs.pop('bins')\n",
    "    else:\n",
    "        minn = 0\n",
    "        maxx = max(c.max(),t.max(),h.max())\n",
    "        delta = (maxx-minn)/nbins\n",
    "        bins = torch.arange(minn,maxx+delta,delta).numpy()\n",
    "    \n",
    "    c = c.cpu().numpy()\n",
    "    t = t.cpu().numpy()\n",
    "    h = h.cpu().numpy()\n",
    "    \n",
    "    ax.hist(c,alpha=0.5,label='ChromoGen',density=True,bins=bins,edgecolor='none',**kwargs)\n",
    "    ax.hist(t,alpha=0.5,label='Dip-C',density=True,bins=bins,edgecolor='none',**kwargs)\n",
    "    ax.hist(h,alpha=0.5,label='Homopolymer',density=True,bins=bins,edgecolor='none',**kwargs)\n",
    "    \n",
    "    ax.set_xlabel('Distance (nm)')# (Bond Lengths)')\n",
    "    ax.set_ylabel('Probability density')\n",
    "    title = 'Euclidean distance distribution for monomers separated by:\\n'\n",
    "    title+= f'{sep} bonds' if sep != 1 else f'{sep} bond'\n",
    "    ax.set_title(title)\n",
    "\n",
    "    if sep == 1:\n",
    "        ax.set_ylim([0,.018])\n",
    "    else:\n",
    "        ax.set_ylim([0,ax.get_ylim()[1]])\n",
    "    #ax.set_yticks(ax.get_ylim())\n",
    "    \n",
    "    ax.set_xlim([bins[0],bins[-1]])\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.set_aspect((xlim[1]-xlim[0])/(ylim[1]-ylim[0]))\n",
    "    \n",
    "    ax.legend(framealpha=0,loc='upper right')#,fontsize='small')\n",
    "\n",
    "    return fig, ax\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3ff2d-358d-4186-961c-dde0a69d0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(nrows=1,ncols=3,layout='constrained',figsize=(7.08661,6.69291/2.5))\n",
    "\n",
    "# Distance histograms\n",
    "compare_dists(1,bins=torch.arange(0,504,5.),fig=fig,ax=axes[0])\n",
    "axes[0].set_title('Monomers separated by\\none bond')\n",
    "compare_dists(50,bins=torch.arange(0,2500,25),fig=fig,ax=axes[1])\n",
    "axes[1].set_title('Monomers separated by\\nfifty bonds')\n",
    "\n",
    "# Distance KL divergence plot\n",
    "ax = axes[2]\n",
    "ax.plot(torch.arange(1,64)*20,cgen_KL,label='ChromoGen')\n",
    "ax.plot(torch.arange(1,64)*20,homo_KL,color='#b3de69',label='Homopolymer')\n",
    "ax.legend(framealpha=0)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Sequence separation (kb)')\n",
    "ax.set_ylabel('KL divergence')\n",
    "ax.set_xlim([-5,1285])\n",
    "ylim = ax.get_ylim()\n",
    "xlim = ax.get_xlim()\n",
    "ax.set_aspect( 2.5*(xlim[1]-xlim[0])/(ylim[1]-ylim[0]), adjustable='box')\n",
    "ax.set_title('Agreement across all\\nsequence separations')\n",
    "\n",
    "fig.savefig(pdf_dir / 'distance_stats_with_homopolymer.pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fa1ad-8092-42e4-a068-3e946f9472f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cgen_KL, homo_KL, homo_dists, dipc_dists, cgen_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4650d9-91fb-46a7-ac73-c5d912f3c214",
   "metadata": {},
   "source": [
    "#### Fig 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd625d29-1fcc-4908-8784-3e022bf73d20",
   "metadata": {},
   "source": [
    "Note: I made this figure with three panels so that its relative font sizes, etc., would be identical to parts a-b. Later combined everything in Adobe Illustrator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc60ec8-e6c5-41bb-8abd-927c39ffec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(cgen_data,dipc_data,homo_data=None,nbins=25,fig=None,ax=None,alpha=0.5,xlim=None):\n",
    "\n",
    "    # Convert to numpy to improve speed in plt \n",
    "    cgen_data = cgen_data.cpu().numpy()\n",
    "    dipc_data = dipc_data.cpu().numpy()\n",
    "\n",
    "    # Create consistent bins for eaach data type\n",
    "    if xlim is None:\n",
    "        minn = min(cgen_data.min(),dipc_data.min())\n",
    "        maxx = max(cgen_data.max(),dipc_data.max())\n",
    "    else:\n",
    "        minn = xlim[0]\n",
    "        maxx = xlim[1]\n",
    "    delta = (maxx-minn)/nbins\n",
    "    bins = torch.arange(minn,maxx+delta/2,delta).numpy()\n",
    "\n",
    "    # Create the figure, if necessary\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "    # Create the histogram\n",
    "    ax.hist(cgen_data,alpha=alpha,bins=bins,label='ChromoGen',edgecolor='none',density=True)\n",
    "    ax.hist(dipc_data,alpha=alpha,bins=bins,label='Dip-C',edgecolor='none',density=True)\n",
    "    if homo_data is not None:\n",
    "        ax.hist(homo_data.cpu().numpy(),alpha=alpha,bins=bins,label='Homopolymer',edgecolor='none',density=True)\n",
    "    ax.set_ylabel('Probability density')\n",
    "\n",
    "    # Make a square panel\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    ylim = ax.get_ylim()\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.set_aspect( (xlim[1]-xlim[0])/(ylim[1]-ylim[0]) )\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd5bf8-ce57-40c7-b78a-70a3d7df9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and multiply by 100 to convert to nm\n",
    "homo_rg = torch.load(homo_rg_fp) * 100\n",
    "dipc_rg = torch.load(dipc_rg_fp) * 100\n",
    "cgen_rg = torch.load(cgen_rg_fp) * 100 \n",
    "\n",
    "fig, axes = plt.subplots(ncols=3,nrows=1,layout='constrained',figsize=(7.08661,6.69291/2.5)) # To ensure equal size as the other figure\n",
    "\n",
    "# R_g histogram\n",
    "ax = axes[0]\n",
    "plot_hist(cgen_rg,dipc_rg,homo_rg,fig=fig,ax=ax,nbins=100,xlim=[200,1000])\n",
    "ax.set_xlabel('$R_g$ (nm)')\n",
    "ax.legend(framealpha=0,loc='upper right')#,fontsize='small')\n",
    "fig.savefig(pdf_dir / 'r_g_with_homopolymer.pdf',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95295e7-7a21-4489-81fd-80ccc9f9731e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
